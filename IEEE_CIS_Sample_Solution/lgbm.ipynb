{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is modified from https://www.kaggle.com/kyakovlev/ieee-lgbm-with-groupkfold-cv as a benchmark.  \n",
    "If convenient, please upvote this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, warnings, random, datetime\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold, GroupKFold, TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import math\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ########################### Model\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# def make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=2):\n",
    "#     #folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "#     folds = GroupKFold(n_splits=NFOLDS)\n",
    "#     X,y = tr_df[features_columns], tr_df[target]    \n",
    "#     P,P_y = tt_df[features_columns], tt_df[target]  \n",
    "\n",
    "#     tt_df = tt_df[['TransactionID',target]]    \n",
    "#     predictions = np.zeros(len(tt_df))\n",
    "    \n",
    "#     for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "#         print('Fold:',fold_)\n",
    "#         tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n",
    "#         vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n",
    "            \n",
    "#         print(len(tr_x),len(vl_x))\n",
    "#         tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "\n",
    "#         if LOCAL_TEST:\n",
    "#             vl_data = lgb.Dataset(P, label=P_y) \n",
    "#         else:\n",
    "#             vl_data = lgb.Dataset(vl_x, label=vl_y)  \n",
    "\n",
    "#         estimator = lgb.train(\n",
    "#             lgb_params,\n",
    "#             tr_data,\n",
    "#             valid_sets = [tr_data, vl_data],\n",
    "#             verbose_eval = 200,\n",
    "#         )   \n",
    "        \n",
    "#         pp_p = estimator.predict(P)\n",
    "#         predictions += pp_p/NFOLDS\n",
    "\n",
    "#         if LOCAL_TEST:\n",
    "#             feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "#             print(feature_imp)\n",
    "# #         feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "# #         print(feature_imp)\n",
    "        \n",
    "#         del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n",
    "#         gc.collect()\n",
    "        \n",
    "#         feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "#         feature_imp.to_csv(\"feature.csv\")\n",
    "#     tt_df['prediction'] = predictions\n",
    "    \n",
    "#     return tt_df\n",
    "# ## -------------------\n",
    "########################### Model\n",
    "import lightgbm as lgb\n",
    "\n",
    "def make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=2):\n",
    "    \n",
    "    #folds = TimeSeriesSplit(n_splits=NFOLDS)\n",
    "    folds = GroupKFold(n_splits=NFOLDS)\n",
    "\n",
    "    X,y = tr_df[features_columns], tr_df[target]    \n",
    "    P,P_y = tt_df[features_columns], tt_df[target]  \n",
    "    split_groups = tr_df['DT_M']\n",
    "\n",
    "    tt_df = tt_df[['TransactionID',target]]    \n",
    "    predictions = np.zeros(len(tt_df))\n",
    "    oof = np.zeros(len(tr_df))\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=split_groups)):\n",
    "        print('Fold:',fold_)\n",
    "        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n",
    "        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n",
    "            \n",
    "        print(len(tr_x),len(vl_x))\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)  \n",
    "\n",
    "        estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            tr_data,\n",
    "            valid_sets = [tr_data, vl_data],\n",
    "            verbose_eval = 200,\n",
    "        )   \n",
    "        \n",
    "        pp_p = estimator.predict(P)\n",
    "        predictions += pp_p/NFOLDS\n",
    "        \n",
    "        oof_preds = estimator.predict(vl_x)\n",
    "        oof[val_idx] = (oof_preds - oof_preds.min())/(oof_preds.max() - oof_preds.min())\n",
    "\n",
    "        if LOCAL_TEST:\n",
    "            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "            print(feature_imp)\n",
    "\n",
    "        feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "        feature_imp.to_csv(\"feature.csv\",index = False)         \n",
    "            \n",
    "        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n",
    "        gc.collect()\n",
    "        \n",
    "    tt_df['prediction'] = predictions\n",
    "    print('OOF AUC:', metrics.roc_auc_score(y, oof))\n",
    "    if LOCAL_TEST:\n",
    "        print('Holdout AUC:', metrics.roc_auc_score(tt_df[TARGET], tt_df['prediction']))\n",
    "    \n",
    "    return tt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = False\n",
    "TARGET = 'isFraud'\n",
    "START_DATE = datetime.datetime.strptime('2017-12-01', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n",
      "Shape control: (590540, 394) (506691, 394)\n"
     ]
    }
   ],
   "source": [
    "########################### DATA LOAD\n",
    "#################################################################################\n",
    "print('Load Data')\n",
    "train_df = pd.read_pickle('data\\train_transaction.pkl')\n",
    "\n",
    "if LOCAL_TEST:\n",
    "    \n",
    "    # Convert TransactionDT to \"Month\" time-period. \n",
    "    # We will also drop penultimate block \n",
    "    # to \"simulate\" test set values difference\n",
    "    train_df['DT_M'] = train_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "    train_df['DT_M'] = (train_df['DT_M'].dt.year-2017)*12 + train_df['DT_M'].dt.month \n",
    "    test_df = train_df[train_df['DT_M']==train_df['DT_M'].max()].reset_index(drop=True)\n",
    "    train_df = train_df[train_df['DT_M']<(train_df['DT_M'].max()-1)].reset_index(drop=True)\n",
    "    \n",
    "    train_identity = pd.read_pickle('data\\train_identity.pkl')\n",
    "    test_identity  = train_identity[train_identity['TransactionID'].isin(\n",
    "                                    test_df['TransactionID'])].reset_index(drop=True)\n",
    "    train_identity = train_identity[train_identity['TransactionID'].isin(\n",
    "                                    train_df['TransactionID'])].reset_index(drop=True)\n",
    "    del train_df['DT_M'], test_df['DT_M']\n",
    "    \n",
    "else:\n",
    "    test_df = pd.read_pickle('data\\test_transaction.pkl')\n",
    "    train_identity = pd.read_pickle('data\\train_identity.pkl')\n",
    "    test_identity = pd.read_pickle('data\\test_identity.pkl')\n",
    "    \n",
    "base_columns = list(train_df) + list(train_identity)\n",
    "print('Shape control:', train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### All features columns\n",
    "#################################################################################\n",
    "## Main Data\n",
    "# 'TransactionID',                     -> This is pure noise, we cannot use this column as feature\n",
    "# 'isFraud',                           -> Our Target\n",
    "# 'TransactionDT',                     -> Time from reference time point. VERY valuable column\n",
    "# 'TransactionAmt',                    -> Many unique values and has to be combined with other columns\n",
    "#                                         The best score boost should come from \n",
    "#                                         TransactionDT->TransactionAmt combination\n",
    "# 'ProductCD',                         -> 100% categorical feature options to use:\n",
    "#                                         Frequency encoding/Target encoding/\n",
    "#                                         Combinations with other columns/Model categorical feature\n",
    "# 'card1' - 'card6',                   -> Categorical features with information about Client\n",
    "# 'addr1' - 'addr2',                   -> add2 - Country / addr1 - subzone\n",
    "# 'dist1' - 'dist2',                   -> dist2 - Country distance / dist1 - local distance from merchant\n",
    "# 'P_emaildomain' - 'R_emaildomain',   -> Categorical feature. It's possible to make \n",
    "#                                         subgroup feature from it or general group\n",
    "# 'C1' - 'C14'                         -> Counts. Should be numerical features (all ints?)\n",
    "# 'D1' - 'D15'                         \n",
    "# 'M1' - 'M9'\n",
    "# 'V1' - 'V339'\n",
    "\n",
    "## Identity Data\n",
    "# 'TransactionID'\n",
    "# 'id_01' - 'id_38'\n",
    "# 'DeviceType',\n",
    "# 'DeviceInfo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['is_na_sum1'] = df.isna().sum(axis = 1)\n",
    "    df['not_na_sum1'] = df.notna().sum(axis = 1)\n",
    "for df in [train_identity,test_identity]:\n",
    "    df['is_na_sum2'] = df.isna().sum(axis = 1)\n",
    "    df['not_na_sum2'] = df.notna().sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for df in [train_df, test_df]:\n",
    "#     df['P_is_protonmail'] = 0\n",
    "#     df['P_is_protonmail'][(df['P_emaildomain'] == 'protonmail.com')|(df['P_emaildomain'] == 'mail.com')|(df['P_emaildomain'] == 'outlook.es')|(df['P_emaildomain'] == 'aim.com')|(df['P_emaildomain'] == 'outlook.com')] = 1\n",
    "#     df['R_is_protonmail'] = 0\n",
    "#     df['R_is_protonmail'][(df['R_emaildomain'] == 'protonmail.com')|(df['R_emaildomain'] == 'mail.com')|(df['R_emaildomain'] == 'netzero.net')|(df['R_emaildomain'] == 'outlook.com')|(df['R_emaildomain'] == 'outlook.es')|(df['R_emaildomain'] == 'icloud.com')|(df['R_emaildomain'] == 'gmail.com')] = 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_df['isFraud'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# emain_domain = set(list(train_df['P_emaildomain'].value_counts().keys()) + list(train_df['R_emaildomain'].value_counts().keys()))\n",
    "# l = []\n",
    "# for emain in emain_domain:\n",
    "#     all_ = (train_df['R_emaildomain'] == emain).sum()\n",
    "#     sample = train_df['isFraud'][train_df['R_emaildomain'] == emain].mean()\n",
    "#     l.append([emain,all_,sample])\n",
    "\n",
    "# l.sort(key = lambda x:x[2],reverse = True)\n",
    "# [i[0] for i in l if i[2] > 0.1]    \n",
    "# l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### D9 and TransactionDT\n",
    "# Let's add temporary \"time variables\" for aggregations\n",
    "# and add normal \"time variables\"\n",
    "\n",
    "# Also, seems that D9 column is an hour\n",
    "# and it is the same as df['DT'].dt.hour\n",
    "for df in [train_df, test_df]:\n",
    "    # Temporary\n",
    "    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n",
    "    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n",
    "    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n",
    "    \n",
    "    df['DT_hour'] = df['DT'].dt.hour\n",
    "    df['DT_day_week'] = df['DT'].dt.dayofweek\n",
    "    df['DT_day'] = df['DT'].dt.day\n",
    "    \n",
    "    # D9 column\n",
    "    df['D9'] = np.where(df['D9'].isna(),0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in [train_df,test_df]:\n",
    "    df['D_re'] = (df['TransactionDT'] / 86400).apply(np.round)\n",
    "    df['first_date'] = df['D_re'] - df['D1']\n",
    "    df['first_date2'] = df['D_re'] - df['D4']\n",
    "    df.drop('D_re',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temp_df = pd.concat([train_df[['DT_hour','isFraud']],test_df[['DT_hour','isFraud']]])\n",
    "# dic = dict()\n",
    "# for i in range(24):\n",
    "#     dic[i] = (temp_df['DT_hour'] == i).sum()\n",
    "# for df in [train_df, test_df]:\n",
    "#     # Temporary\n",
    "#     df['TransactionPerHour'] = df['DT_hour'].map(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_df = pd.concat([train_df[['TransactionDT','isFraud']],test_df[['TransactionDT','isFraud']]])\n",
    "temp_df['hours'] = temp_df['TransactionDT'] // 3600\n",
    "groups = temp_df.groupby('hours')\n",
    "dic = dict()\n",
    "for name,group in groups:\n",
    "    dic[name] = group.count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['hours'] = df['TransactionDT'] // 3600\n",
    "    df['TransactionPerHour'] = df['hours'].map(dic)\n",
    "    df.drop(['hours'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card1\n",
      "DeviceInfo\n"
     ]
    }
   ],
   "source": [
    "########################### Reset values for \"noise\" card1\n",
    "i_cols = ['card1']\n",
    "\n",
    "for col in i_cols: \n",
    "    print(col)\n",
    "    valid_card = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    valid_card = valid_card[col].value_counts()\n",
    "    valid_card = valid_card[valid_card>2]\n",
    "    valid_card = list(valid_card.index)\n",
    "\n",
    "    train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n",
    "    test_df[col]  = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n",
    "\n",
    "    train_df[col] = np.where(train_df[col].isin(valid_card), train_df[col], np.nan)\n",
    "    test_df[col]  = np.where(test_df[col].isin(valid_card), test_df[col], np.nan)\n",
    "    \n",
    "i_cols = ['DeviceInfo']    \n",
    "for col in i_cols: \n",
    "    print(col)\n",
    "    valid_card = pd.concat([train_identity[[col]], test_identity[[col]]])\n",
    "    valid_card = valid_card[col].value_counts()\n",
    "    valid_card = valid_card[valid_card>2]\n",
    "    valid_card = list(valid_card.index)\n",
    "\n",
    "    train_identity[col] = np.where(train_identity[col].isin(test_identity[col]), train_identity[col], np.nan)\n",
    "    test_identity[col]  = np.where(test_identity[col].isin(train_identity[col]), test_identity[col], np.nan)\n",
    "\n",
    "    train_identity[col] = np.where(train_identity[col].isin(valid_card), train_identity[col], np.nan)\n",
    "    test_identity[col]  = np.where(test_identity[col].isin(valid_card), test_identity[col], np.nan)\n",
    "    \n",
    "for col in ['card2','card3','card4','card5','card6',]: \n",
    "    train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n",
    "    test_df[col]  = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### M columns (except M4)\n",
    "# All these columns are binary encoded 1/0\n",
    "# We can have some features from it\n",
    "i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    df['M_sum'] = df[i_cols].sum(axis=1).astype(np.int8)\n",
    "    df['M_na'] = df[i_cols].isna().sum(axis=1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ########################### ProductCD and M4 Target mean\n",
    "# #for col in ['ProductCD','M4','P_emaildomain','R_emaildomain']:\n",
    "# for col in ['ProductCD','M4']:\n",
    "#     temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n",
    "#                                                         columns={'mean': col+'_target_mean'})\n",
    "#     temp_dict.index = temp_dict[col].values\n",
    "#     temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "\n",
    "#     train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n",
    "#     test_df[col+'_target_mean']  = test_df[col].map(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#经cv和lb都检验无效，暂时删去\n",
    "# for df in [train_df,test_df]:\n",
    "#     df['TransactionAmt_decimal'] = ((df['TransactionAmt'] - df['TransactionAmt'].astype(int)) * 1000).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def values_normalization(dt_df, periods, columns):\n",
    "#     for period in periods:\n",
    "#         for col in columns:\n",
    "#             new_col = col +'_'+ period\n",
    "#             dt_df[col] = dt_df[col].astype(float)  \n",
    "\n",
    "#             temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
    "#             temp_min.index = temp_min[period].values\n",
    "#             temp_min = temp_min['min'].to_dict()\n",
    "\n",
    "#             temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
    "#             temp_max.index = temp_max[period].values\n",
    "#             temp_max = temp_max['max'].to_dict()\n",
    "\n",
    "#             temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n",
    "#             temp_mean.index = temp_mean[period].values\n",
    "#             temp_mean = temp_mean['mean'].to_dict()\n",
    "\n",
    "#             temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n",
    "#             temp_std.index = temp_std[period].values\n",
    "#             temp_std = temp_std['std'].to_dict()\n",
    "\n",
    "#             dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
    "#             dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
    "#             dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n",
    "#             dt_df['temp_std'] = dt_df[period].map(temp_std)\n",
    "\n",
    "#             dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
    "#             dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])/(dt_df['temp_std'])\n",
    "#             del dt_df['temp_min'],dt_df['temp_max'],dt_df['temp_mean'],dt_df['temp_std']\n",
    "#     return dt_df\n",
    "# periods = ['DT_D','DT_W','DT_M']\n",
    "# for df in [train_df, test_df]:\n",
    "#     df = values_normalization(df, periods, ['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### Let's play \"sudoku\" and fill nans in cards columns\n",
    "#以card1作为基准，来补全其他card\n",
    "i_cols = ['TransactionID','card1','card2','card3','card4','card5','card6']\n",
    "\n",
    "full_df = pd.concat([train_df[i_cols], test_df[i_cols]])\n",
    "\n",
    "## I've used frequency encoding before so we have ints here\n",
    "## we will drop very rare cards\n",
    "full_df['card6'] = np.where(full_df['card6']==30, np.nan, full_df['card6'])\n",
    "full_df['card6'] = np.where(full_df['card6']==16, np.nan, full_df['card6'])\n",
    "\n",
    "i_cols = ['card2','card3','card4','card5','card6']\n",
    "\n",
    "## We will find best match for nan values and fill with it\n",
    "for col in i_cols:\n",
    "    temp_df = full_df.groupby(['card1',col])[col].agg(['count']).reset_index()\n",
    "    temp_df = temp_df.sort_values(by=['card1','count'], ascending=False).reset_index(drop=True)\n",
    "    del temp_df['count']\n",
    "    temp_df = temp_df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    temp_df.index = temp_df['card1'].values\n",
    "    temp_df = temp_df[col].to_dict()\n",
    "    full_df[col] = np.where(full_df[col].isna(), full_df['card1'].map(temp_df), full_df[col])\n",
    "    \n",
    "    \n",
    "i_cols = ['card1','card2','card3','card4','card5','card6']\n",
    "for col in i_cols:\n",
    "    train_df[col] = full_df[full_df['TransactionID'].isin(train_df['TransactionID'])][col].values\n",
    "    test_df[col] = full_df[full_df['TransactionID'].isin(test_df['TransactionID'])][col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V307', 'card1']\n",
      "['V307', 'card2']\n",
      "['V307', 'card3']\n",
      "['V307', 'card5']\n",
      "['V307', 'uid']\n",
      "['V307', 'uid2']\n",
      "['V307', 'uid3']\n",
      "['V307', 'uid4']\n",
      "['V307', 'uid5']\n",
      "['V307', 'addr1']\n",
      "['V307', 'uid7']\n",
      "['V307', 'uid8']\n",
      "['TransactionAmt', 'card1']\n",
      "['TransactionAmt', 'card2']\n",
      "['TransactionAmt', 'card3']\n",
      "['TransactionAmt', 'card5']\n",
      "['TransactionAmt', 'uid']\n",
      "['TransactionAmt', 'uid2']\n",
      "['TransactionAmt', 'uid3']\n",
      "['TransactionAmt', 'uid4']\n",
      "['TransactionAmt', 'uid5']\n",
      "['TransactionAmt', 'addr1']\n",
      "['TransactionAmt', 'uid7']\n",
      "['TransactionAmt', 'uid8']\n",
      "['D15', 'card1']\n",
      "['D15', 'card2']\n",
      "['D15', 'card3']\n",
      "['D15', 'card5']\n",
      "['D15', 'uid']\n",
      "['D15', 'uid2']\n",
      "['D15', 'uid3']\n",
      "['D15', 'uid4']\n",
      "['D15', 'uid5']\n",
      "['D15', 'addr1']\n",
      "['D15', 'uid7']\n",
      "['D15', 'uid8']\n",
      "['dist1', 'card1']\n",
      "['dist1', 'card2']\n",
      "['dist1', 'card3']\n",
      "['dist1', 'card5']\n",
      "['dist1', 'uid']\n",
      "['dist1', 'uid2']\n",
      "['dist1', 'uid3']\n",
      "['dist1', 'uid4']\n",
      "['dist1', 'uid5']\n",
      "['dist1', 'addr1']\n",
      "['dist1', 'uid7']\n",
      "['dist1', 'uid8']\n"
     ]
    }
   ],
   "source": [
    "########################### TransactionAmt\n",
    "\n",
    "# Let's add some kind of client uID based on cardID ad addr columns\n",
    "# The value will be very specific for each client so we need to remove it\n",
    "# from final feature. But we can use it for aggregations.\n",
    "train_df['uid'] = train_df['card1'].astype(str)+'_'+train_df['card2'].astype(str)\n",
    "test_df['uid'] = test_df['card1'].astype(str)+'_'+test_df['card2'].astype(str)\n",
    "\n",
    "train_df['uid2'] = train_df['uid'].astype(str)+'_'+train_df['card3'].astype(str)+'_'+train_df['card5'].astype(str)\n",
    "test_df['uid2'] = test_df['uid'].astype(str)+'_'+test_df['card3'].astype(str)+'_'+test_df['card5'].astype(str)\n",
    "\n",
    "train_df['uid3'] = train_df['uid2'].astype(str)+'_'+train_df['addr1'].astype(str)+'_'+train_df['addr2'].astype(str)\n",
    "test_df['uid3'] = test_df['uid2'].astype(str)+'_'+test_df['addr1'].astype(str)+'_'+test_df['addr2'].astype(str)\n",
    "\n",
    "##\n",
    "train_df['uid4'] = train_df['uid3'].astype(str)+'_'+train_df['P_emaildomain'].astype(str)\n",
    "test_df['uid4'] = test_df['uid3'].astype(str)+'_'+test_df['P_emaildomain'].astype(str)\n",
    "\n",
    "train_df['uid5'] = train_df['uid3'].astype(str)+'_'+train_df['R_emaildomain'].astype(str)\n",
    "test_df['uid5'] = test_df['uid3'].astype(str)+'_'+test_df['R_emaildomain'].astype(str)\n",
    "\n",
    "train_df['uid7'] = train_df['card1'].astype(str)+'_'+train_df['addr1'].astype(str)+'_'+train_df['addr2'].astype(str)+'_'+train_df['P_emaildomain'].astype(str)+'_'+train_df['first_date'].astype(str)\n",
    "test_df['uid7'] = test_df['card1'].astype(str)+'_'+test_df['addr1'].astype(str)+'_'+test_df['addr2'].astype(str)+'_'+test_df['P_emaildomain'].astype(str)+'_'+test_df['first_date'].astype(str)\n",
    "\n",
    "train_df['uid8'] = train_df['card1'].astype(str)+'_'+train_df['addr1'].astype(str)+'_'+train_df['addr2'].astype(str)+'_'+train_df['P_emaildomain'].astype(str)+'_'+train_df['first_date2'].astype(str)\n",
    "test_df['uid8'] = test_df['card1'].astype(str)+'_'+test_df['addr1'].astype(str)+'_'+test_df['addr2'].astype(str)+'_'+test_df['P_emaildomain'].astype(str)+'_'+test_df['first_date2'].astype(str)\n",
    "\n",
    "##\n",
    "\n",
    "# Check if the Transaction Amount is common or not (we can use freq encoding here)\n",
    "# In our dialog with a model we are telling to trust or not to these values   \n",
    "train_df['TransactionAmt_check'] = np.where(train_df['TransactionAmt'].isin(test_df['TransactionAmt']), 1, 0)\n",
    "test_df['TransactionAmt_check']  = np.where(test_df['TransactionAmt'].isin(train_df['TransactionAmt']), 1, 0)\n",
    "\n",
    "# For our model current TransactionAmt is a noise\n",
    "# https://www.kaggle.com/kyakovlev/ieee-check-noise\n",
    "# (even if features importances are telling contrariwise)\n",
    "# There are many unique values and model doesn't generalize well\n",
    "# Lets do some aggregations\n",
    "i_cols = ['card1','card2','card3','card5','uid','uid2','uid3','uid4','uid5','addr1','uid7','uid8']\n",
    "data_cols = ['V307','TransactionAmt','D15','dist1']\n",
    "#data_cols = ['TransactionAmt','D15','dist1','D4']\n",
    "#data_cols = ['TransactionAmt']\n",
    "\n",
    "for col_data in data_cols:\n",
    "    for col in i_cols:\n",
    "        print([col_data,col])\n",
    "        for agg_type in ['mean', 'std','max','min','median','nunique','skew']:\n",
    "            new_col_name = col+'_' + col_data + '_' +agg_type\n",
    "            temp_df = pd.concat([train_df[[col, col_data]], test_df[[col,col_data]]])\n",
    "            temp_df = temp_df.groupby([col])[col_data].agg([agg_type]).reset_index().rename(\n",
    "                                                    columns={agg_type: new_col_name})\n",
    "\n",
    "            temp_df.index = list(temp_df[col])\n",
    "            temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "            train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "            test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for col in i_cols:\n",
    "#     new_col_name = col+'_TransactionAmt_'+ \"cumsum\"\n",
    "#     temp_df = pd.concat([train_df[[col, 'TransactionAmt']], test_df[[col,'TransactionAmt']]])\n",
    "#     #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n",
    "#     temp = temp_df.groupby(col)['TransactionAmt'].cumsum()\n",
    "\n",
    "#     train_df[new_col_name] = temp.iloc[:train_df.shape[0]]\n",
    "#     test_df[new_col_name]  = temp.iloc[train_df.shape[0]:] \n",
    "\n",
    "#     new_col_name = col+'_TransactionAmt_'+ \"cumcount\"\n",
    "#     temp_df = pd.concat([train_df[[col, 'TransactionAmt']], test_df[[col,'TransactionAmt']]])\n",
    "#     #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n",
    "#     temp = temp_df.groupby(col)['TransactionAmt'].cumcount()\n",
    "\n",
    "#     train_df[new_col_name] = temp.iloc[:train_df.shape[0]]\n",
    "#     test_df[new_col_name]  = temp.iloc[train_df.shape[0]:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Small \"hack\" to transform distribution \n",
    "# (doesn't affect auc much, but I like it more)\n",
    "# please see how distribution transformation can boost your score \n",
    "# (not our case but related)\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_transformed_target.html\n",
    "train_df['TransactionAmt'] = np.log1p(train_df['TransactionAmt'])\n",
    "test_df['TransactionAmt'] = np.log1p(test_df['TransactionAmt']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### 'P_emaildomain' - 'R_emaildomain'\n",
    "p = 'P_emaildomain'\n",
    "r = 'R_emaildomain'\n",
    "uknown = 'email_not_provided'\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    df[p] = df[p].fillna(uknown)\n",
    "    df[r] = df[r].fillna(uknown)\n",
    "    \n",
    "    # Check if P_emaildomain matches R_emaildomain\n",
    "    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=uknown),1,0)\n",
    "\n",
    "    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n",
    "    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "## Local test doesn't show any boost here, \n",
    "## but I think it's a good option for model stability \n",
    "\n",
    "## Also, we will do frequency encoding later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#试一下对Email的清理有没有用\n",
    "#\n",
    "email_dic = {\n",
    "'yahoo'  : 'Yahoo',\n",
    "'ymail'  : 'Yahoo',\n",
    "'frontier'  : 'Yahoo',\n",
    "'rocketmail' : 'Yahoo',\n",
    "'hotmail': 'Microsoft',\n",
    "'outlook' : 'Microsoft',\n",
    "'live' : 'Microsoft',\n",
    "'msn': 'Microsoft',\n",
    "'icloud'  : 'Appe',\n",
    "'mac'  : 'Appe',\n",
    "'me' : 'Appe',\n",
    "'prodigy' : 'AT&T',\n",
    "'att' : 'AT&T',\n",
    "'sbcglobal': 'AT&T',\n",
    "'centurylink' :'Centurylink',\n",
    "'embarqmail':'Centurylink', \n",
    "'q' :'Centurylink',\n",
    "'aim' : 'AOL',\n",
    "'aol': 'AOL',\n",
    "'twc'  : 'Spectrum',\n",
    "'charter' : 'Spectrum',\n",
    "'email_not_provided':'other'\n",
    "}\n",
    "train_df['R_emaildomain_prefix2'] = train_df['R_emaildomain_prefix'].map(email_dic)\n",
    "train_df['P_emaildomain_prefix2'] = train_df['P_emaildomain_prefix'].map(email_dic)\n",
    "test_df['R_emaildomain_prefix2'] = test_df['R_emaildomain_prefix'].map(email_dic)\n",
    "test_df['P_emaildomain_prefix2'] = test_df['P_emaildomain_prefix'].map(email_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in [train_df,test_df]:\n",
    "    df['R_emaildomain_prefix2'][df['R_emaildomain_prefix2'].isnull() == True] = df['R_emaildomain_prefix']\n",
    "    df['P_emaildomain_prefix2'][df['P_emaildomain_prefix2'].isnull() == True] = df['P_emaildomain_prefix']\n",
    "    df.drop('R_emaildomain_prefix',axis = 1,inplace = True)\n",
    "    df.drop('P_emaildomain_prefix',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### Device info\n",
    "for df in [train_identity, test_identity]:\n",
    "    ########################### Device info\n",
    "    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "    df['DeviceInfo_device'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "    df['DeviceInfo_version'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "    \n",
    "    ########################### Device info 2\n",
    "    df['id_30'] = df['id_30'].fillna('unknown_device').str.lower()\n",
    "    df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "    df['id_30_version'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "    \n",
    "    ########################### Browser\n",
    "    df['id_31'] = df['id_31'].fillna('unknown_device').str.lower()\n",
    "    df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in [train_identity, test_identity]:\n",
    "    df['UserAgent'] = np.where(df['DeviceInfo'].str.contains(\"/\"),df['DeviceInfo'],np.nan)\n",
    "    df['ParsingError'] = np.where(df['DeviceInfo'].str.contains(\"/\"),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### Merge Identity columns\n",
    "temp_df = train_df[['TransactionID']]\n",
    "temp_df = temp_df.merge(train_identity, on=['TransactionID'], how='left')\n",
    "del temp_df['TransactionID']\n",
    "train_df = pd.concat([train_df,temp_df], axis=1)\n",
    "    \n",
    "temp_df = test_df[['TransactionID']]\n",
    "temp_df = temp_df.merge(test_identity, on=['TransactionID'], how='left')\n",
    "del temp_df['TransactionID']\n",
    "test_df = pd.concat([test_df,temp_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in [train_df,test_df]:\n",
    "    df['is_na_sum'] = df['is_na_sum1'] + df['is_na_sum2']\n",
    "    df['not_na_sum'] = df['not_na_sum1'] + df['not_na_sum2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for col in ['DeviceInfo']:\n",
    "#     temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n",
    "#                                                         columns={'mean': col+'_target_mean'})\n",
    "#     temp_dict.index = temp_dict[col].values\n",
    "#     temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "\n",
    "#     train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n",
    "#     test_df[col+'_target_mean']  = test_df[col].map(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# browser = pd.read_csv(r'D:\\学习\\data_mining\\IEEE-CIS_Fraud_Detection\\kernels\\9468kernel\\brower_version.csv',engine = 'python')\n",
    "# dic = pd.Series(browser[browser.columns[2]].values, index=browser[browser.columns[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_df['BrowserUpToDate'] = train_df['id_31'].map(dic)\n",
    "# train_df['BrowserUpToDate'] = pd.to_datetime(train_df['BrowserUpToDate'])\n",
    "# train_df['WhetherBrowserUpToDate'] = train_df['DT'] <= train_df['BrowserUpToDate']\n",
    "# train_df['WhetherBrowserUpToDate'][train_df['BrowserUpToDate'].isnull()] = np.nan\n",
    "# train_df['WhetherBrowserUpToDate'][(train_df['id_31'].str.contains(\"ie\") == True) & (train_df['id_31'] != 'android webview 4.0')] = 0\n",
    "# train_df.drop(['BrowserUpToDate'],axis = 1,inplace = True)\n",
    "\n",
    "# test_df['BrowserUpToDate'] = test_df['id_31'].map(dic)\n",
    "# test_df['BrowserUpToDate'] = pd.to_datetime(test_df['BrowserUpToDate'])\n",
    "# test_df['WhetherBrowserUpToDate'] = test_df['DT'] <= test_df['BrowserUpToDate']\n",
    "# test_df['WhetherBrowserUpToDate'][test_df['BrowserUpToDate'].isnull()] = np.nan\n",
    "# test_df['WhetherBrowserUpToDate'][(test_df['id_31'].str.contains(\"ie\") == True) & (test_df['id_31'] != 'android webview 4.0')] = 0\n",
    "# test_df.drop(['BrowserUpToDate'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### Freq encoding\n",
    "i_cols = ['card1','card2','card3','card5',\n",
    "          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "          'D1','D2','D3','D4','D5','D6','D7','D8',\n",
    "          'addr1','addr2',\n",
    "          'dist1','dist2',\n",
    "#          'P_emaildomain', 'R_emaildomain','R_emaildomain_prefix2','P_emaildomain_prefix2','R_emaildomain_prefix','P_emaildomain_prefix',\n",
    "          'P_emaildomain', 'R_emaildomain','R_emaildomain_prefix2','P_emaildomain_prefix2',\n",
    "          'DeviceInfo','DeviceInfo_device','DeviceInfo_version',\n",
    "          'id_30','id_30_device','id_30_version',\n",
    "          'id_31_device',\n",
    "          'id_33',\n",
    "          'uid','uid2','uid3','uid4','uid5','uid7','uid8'\n",
    "         ]\n",
    "\n",
    "for col in i_cols:\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n",
    "    train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n",
    "    test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)\n",
    "\n",
    "\n",
    "for col in ['DT_M','DT_W','DT_D']:\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()\n",
    "            \n",
    "    train_df[col+'_total'] = train_df[col].map(fq_encode)\n",
    "    test_df[col+'_total']  = test_df[col].map(fq_encode)\n",
    "        \n",
    "\n",
    "periods = ['DT_M','DT_W','DT_D']\n",
    "i_cols = ['uid']\n",
    "for period in periods:\n",
    "    for col in i_cols:\n",
    "        new_column = col + '_' + period\n",
    "            \n",
    "        temp_df = pd.concat([train_df[[col,period]], test_df[[col,period]]])\n",
    "        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n",
    "        fq_encode = temp_df[new_column].value_counts().to_dict()\n",
    "            \n",
    "        train_df[new_column] = (train_df[col].astype(str) + '_' + train_df[period].astype(str)).map(fq_encode)\n",
    "        test_df[new_column]  = (test_df[col].astype(str) + '_' + test_df[period].astype(str)).map(fq_encode)\n",
    "        \n",
    "        train_df[new_column] /= train_df[period+'_total']\n",
    "        test_df[new_column]  /= test_df[period+'_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面特征参考https://www.kaggle.com/gunesevitan/lightgbm-some-new-features/comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATE_COLS = ['D{}'.format(i) for i in range(1, 16) if i != 9]\n",
    "train_df['UniqueDates'] = train_df[DATE_COLS].nunique(axis=1)\n",
    "test_df['UniqueDates'] = test_df[DATE_COLS].nunique(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 864)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506691, 864)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##加上特征转换的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# columns = [i for i in train_df if 'V' in i]\n",
    "# train_df.drop(columns,axis = 1,inplace = True)\n",
    "# test_df.drop(columns,axis = 1,inplace = True)\n",
    "# v_train = pd.read_pickle(r'D:\\学习\\data_mining\\IEEE-CIS_Fraud_Detection\\kernels\\feature_selection\\feature_V_train.pkl')\n",
    "# v_test = pd.read_pickle(r'D:\\学习\\data_mining\\IEEE-CIS_Fraud_Detection\\kernels\\feature_selection\\feature_V_test.pkl')\n",
    "# train_df = pd.concat([train_df,v_train],axis = 1)\n",
    "# test_df = pd.concat([test_df,v_test.reset_index(drop=True)],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#columns2 = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'ProductCD_target_mean', 'M4_target_mean', 'TransactionAmt_check',  'email_check', 'R_emaildomain_prefix2', 'P_emaildomain_prefix2', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_32', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'id_33_0', 'id_33_1', 'is_na_sum2', 'not_na_sum2', 'DeviceInfo_device', 'id_30_device', 'UserAgent',  'ParsingError', 'card1_fq_enc', 'card2_fq_enc', 'card3_fq_enc', 'card5_fq_enc', 'C1_fq_enc', 'C2_fq_enc', 'C3_fq_enc', 'C4_fq_enc', 'C5_fq_enc', 'C6_fq_enc', 'C7_fq_enc', 'C8_fq_enc', 'C9_fq_enc', 'C10_fq_enc', 'C11_fq_enc', 'C13_fq_enc', 'C14_fq_enc', 'D1_fq_enc', 'D2_fq_enc', 'D8_fq_enc', 'addr1_fq_enc', 'addr2_fq_enc', 'dist1_fq_enc', 'dist2_fq_enc', 'P_emaildomain_fq_enc', 'R_emaildomain_fq_enc', 'R_emaildomain_prefix2_fq_enc', 'P_emaildomain_prefix2_fq_enc', 'DeviceInfo_fq_enc', 'DeviceInfo_device_fq_enc', 'DeviceInfo_version_fq_enc', 'id_30_fq_enc', 'id_30_device_fq_enc', 'id_30_version_fq_enc', 'id_31_device_fq_enc', 'id_33_fq_enc', 'uid_fq_enc', 'uid2_fq_enc', 'uid3_fq_enc', 'uid4_fq_enc', 'uid5_fq_enc', 'uid_DT_M', 'uid_DT_W']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#len(drop_cols) / len(columns2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去掉一些用处不大的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_cols = ['id_27','id_22','id_23','id_08','TransactionAmt_check','id_10','D9','dist2_fq_enc','id_24','id_11','id_21','id_15','R_emaildomain_fq_enc','DeviceInfo_fq_enc','ParsingError','id_18','id_25','DeviceInfo_device','M2','id_03','id_09','D13']\n",
    "for df in [train_df, test_df]:\n",
    "    df.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_emaildomain\n",
      "R_emaildomain\n",
      "uid\n",
      "uid2\n",
      "uid3\n",
      "uid4\n",
      "uid5\n",
      "uid7\n",
      "uid8\n",
      "R_emaildomain_prefix2\n",
      "P_emaildomain_prefix2\n",
      "id_30\n",
      "id_31\n",
      "DeviceType\n",
      "DeviceInfo\n",
      "DeviceInfo_version\n",
      "id_30_device\n",
      "id_30_version\n",
      "id_31_device\n",
      "UserAgent\n"
     ]
    }
   ],
   "source": [
    "########################### Encode Str columns\n",
    "# For all such columns (probably not)\n",
    "# we already did frequency encoding (numeric feature)\n",
    "# so we will use astype('category') here\n",
    "for col in list(train_df):\n",
    "    if train_df[col].dtype=='O':\n",
    "        print(col)\n",
    "        train_df[col] = train_df[col].fillna('unseen_before_label')\n",
    "        test_df[col]  = test_df[col].fillna('unseen_before_label')\n",
    "        \n",
    "        train_df[col] = train_df[col].astype(str)\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "        \n",
    "# #######################对这些类别特征全部加上低频过滤#########################\n",
    "# 没有必要。要么删了，要么本来类别就很少\n",
    "#         valid_card = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "#         valid_card = valid_card[col].value_counts()\n",
    "#         valid_card = valid_card[valid_card>2]\n",
    "#         valid_card = list(valid_card.index)\n",
    "\n",
    "#         train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n",
    "#         test_df[col]  = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n",
    "\n",
    "#         train_df[col] = np.where(train_df[col].isin(valid_card), train_df[col], np.nan)\n",
    "#         test_df[col]  = np.where(test_df[col].isin(valid_card), test_df[col], np.nan)\n",
    "        \n",
    "# ###############################################################################        \n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[col])+list(test_df[col]))\n",
    "        train_df[col] = le.transform(train_df[col])\n",
    "        test_df[col]  = le.transform(test_df[col])\n",
    "        \n",
    "        train_df[col] = train_df[col].astype('int')\n",
    "        test_df[col] = test_df[col].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### Model Features \n",
    "## We can use set().difference() but the order matters\n",
    "## Matters only for deterministic results\n",
    "## In case of remove() we will not change order\n",
    "## even if variable will be renamed\n",
    "## please see this link to see how set is ordered\n",
    "## https://stackoverflow.com/questions/12165200/order-of-unordered-python-sets\n",
    "rm_cols = [\n",
    "    'TransactionID','TransactionDT', # These columns are pure noise right now\n",
    "    TARGET,                          # Not target in features))\n",
    "    'uid','uid2','uid3', 'uid4','uid5', 'uid7',           # Our new client uID -> very noisy data\n",
    "    'bank_type',                     # Victims bank could differ by time\n",
    "    'DT','DT_M','DT_W','DT_D',       # Temporary Variables\n",
    "    'DT_hour','DT_day_week','DT_day',\n",
    "    'DT_D_total','DT_W_total','DT_M_total',\n",
    "    'id_30','id_31','id_33','uid8'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M_sum', 'uid8_TransactionAmt_nunique', 'D7_fq_enc', 'first_date', 'uid8_D15_mean', 'uid8_dist1_std', 'M_na', 'uid7_D15_max', 'uid_DT_D', 'uid_DT_W', 'D5_fq_enc', 'uid7_dist1_max', 'uid8_D15_nunique', 'first_date2', 'uid7_dist1_skew', 'is_na_sum', 'uid8_dist1_skew', 'uid7_dist1_mean', 'uid8_dist1_min', 'D6_fq_enc', 'uid8_D15_max', 'uid8_dist1_nunique', 'not_na_sum', 'uid7_D15_std', 'UniqueDates', 'uid7_D15_mean', 'uid8_D15_median', 'uid7_D15_nunique', 'uid5_D15_max', 'TransactionPerHour', 'uid7_D15_median', 'uid7_D15_min', 'uid5_D15_min', 'is_na_sum1', 'uid8_dist1_mean', 'uid7_dist1_min', 'D4_fq_enc', 'uid8_TransactionAmt_max', 'not_na_sum1', 'uid8_dist1_median', 'C12_fq_enc', 'uid7_dist1_median', 'uid8_dist1_max', 'uid5_D15_mean', 'id_31_device', 'uid4_D15_mean', 'uid5_D15_median', 'uid8_D15_std', 'uid7_dist1_nunique', 'uid5_D15_nunique', 'DeviceInfo_version', 'uid8_D15_min', 'D3_fq_enc', 'id_30_version', 'uid4_D15_median']\n"
     ]
    }
   ],
   "source": [
    "########################### Features elimination \n",
    "from scipy.stats import ks_2samp\n",
    "features_check = []\n",
    "columns_to_check = set(list(train_df)).difference(base_columns+rm_cols)\n",
    "for i in columns_to_check:\n",
    "    features_check.append(ks_2samp(test_df[i], train_df[i])[1])\n",
    "\n",
    "features_check = pd.Series(features_check, index=columns_to_check).sort_values() \n",
    "features_discard = list(features_check[features_check==0].index)\n",
    "# if 'is_na_sum' in features_discard:\n",
    "#     features_discard.remove('is_na_sum')\n",
    "# if 'TransactionPerHour' in features_discard:\n",
    "#     features_discard.remove('TransactionPerHour') \n",
    "features_discard2 = []\n",
    "for i in features_discard:\n",
    "    features_discard2.append(i)\n",
    "features_discard = features_discard2\n",
    "print(features_discard)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will reset this list for now (use local test drop),\n",
    "# Good droping will be in other kernels\n",
    "# with better checking\n",
    "#features_discard = [] \n",
    "\n",
    "# Final features list\n",
    "features_columns = [col for col in list(train_df) if col not in rm_cols + features_discard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_df.to_pickle(r\"D:\\学习\\data_mining\\IEEE-CIS_Fraud_Detection\\super_stacking\\data\\lgb\\train_df.pkl\")\n",
    "# test_df.to_pickle(r\"D:\\学习\\data_mining\\IEEE-CIS_Fraud_Detection\\super_stacking\\data\\lgb\\test_df.pkl\")\n",
    "# with open(r\"D:\\学习\\data_mining\\IEEE-CIS_Fraud_Detection\\super_stacking\\data\\lgb\\features_columns\",'w') as f:\n",
    "#     for element in features_columns:\n",
    "#         f.write(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TransactionAmt',\n",
       " 'ProductCD',\n",
       " 'card1',\n",
       " 'card2',\n",
       " 'card3',\n",
       " 'card4',\n",
       " 'card5',\n",
       " 'card6',\n",
       " 'addr1',\n",
       " 'addr2',\n",
       " 'dist1',\n",
       " 'dist2',\n",
       " 'P_emaildomain',\n",
       " 'R_emaildomain',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'D1',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D6',\n",
       " 'D7',\n",
       " 'D8',\n",
       " 'D10',\n",
       " 'D11',\n",
       " 'D12',\n",
       " 'D14',\n",
       " 'D15',\n",
       " 'M1',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'M7',\n",
       " 'M8',\n",
       " 'M9',\n",
       " 'V1',\n",
       " 'V2',\n",
       " 'V3',\n",
       " 'V4',\n",
       " 'V5',\n",
       " 'V6',\n",
       " 'V7',\n",
       " 'V8',\n",
       " 'V9',\n",
       " 'V10',\n",
       " 'V11',\n",
       " 'V12',\n",
       " 'V13',\n",
       " 'V14',\n",
       " 'V15',\n",
       " 'V16',\n",
       " 'V17',\n",
       " 'V18',\n",
       " 'V19',\n",
       " 'V20',\n",
       " 'V21',\n",
       " 'V22',\n",
       " 'V23',\n",
       " 'V24',\n",
       " 'V25',\n",
       " 'V26',\n",
       " 'V27',\n",
       " 'V28',\n",
       " 'V29',\n",
       " 'V30',\n",
       " 'V31',\n",
       " 'V32',\n",
       " 'V33',\n",
       " 'V34',\n",
       " 'V35',\n",
       " 'V36',\n",
       " 'V37',\n",
       " 'V38',\n",
       " 'V39',\n",
       " 'V40',\n",
       " 'V41',\n",
       " 'V42',\n",
       " 'V43',\n",
       " 'V44',\n",
       " 'V45',\n",
       " 'V46',\n",
       " 'V47',\n",
       " 'V48',\n",
       " 'V49',\n",
       " 'V50',\n",
       " 'V51',\n",
       " 'V52',\n",
       " 'V53',\n",
       " 'V54',\n",
       " 'V55',\n",
       " 'V56',\n",
       " 'V57',\n",
       " 'V58',\n",
       " 'V59',\n",
       " 'V60',\n",
       " 'V61',\n",
       " 'V62',\n",
       " 'V63',\n",
       " 'V64',\n",
       " 'V65',\n",
       " 'V66',\n",
       " 'V67',\n",
       " 'V68',\n",
       " 'V69',\n",
       " 'V70',\n",
       " 'V71',\n",
       " 'V72',\n",
       " 'V73',\n",
       " 'V74',\n",
       " 'V75',\n",
       " 'V76',\n",
       " 'V77',\n",
       " 'V78',\n",
       " 'V79',\n",
       " 'V80',\n",
       " 'V81',\n",
       " 'V82',\n",
       " 'V83',\n",
       " 'V84',\n",
       " 'V85',\n",
       " 'V86',\n",
       " 'V87',\n",
       " 'V88',\n",
       " 'V89',\n",
       " 'V90',\n",
       " 'V91',\n",
       " 'V92',\n",
       " 'V93',\n",
       " 'V94',\n",
       " 'V95',\n",
       " 'V96',\n",
       " 'V97',\n",
       " 'V98',\n",
       " 'V99',\n",
       " 'V100',\n",
       " 'V101',\n",
       " 'V102',\n",
       " 'V103',\n",
       " 'V104',\n",
       " 'V105',\n",
       " 'V106',\n",
       " 'V107',\n",
       " 'V108',\n",
       " 'V109',\n",
       " 'V110',\n",
       " 'V111',\n",
       " 'V112',\n",
       " 'V113',\n",
       " 'V114',\n",
       " 'V115',\n",
       " 'V116',\n",
       " 'V117',\n",
       " 'V118',\n",
       " 'V119',\n",
       " 'V120',\n",
       " 'V121',\n",
       " 'V122',\n",
       " 'V123',\n",
       " 'V124',\n",
       " 'V125',\n",
       " 'V126',\n",
       " 'V127',\n",
       " 'V128',\n",
       " 'V129',\n",
       " 'V130',\n",
       " 'V131',\n",
       " 'V132',\n",
       " 'V133',\n",
       " 'V134',\n",
       " 'V135',\n",
       " 'V136',\n",
       " 'V137',\n",
       " 'V138',\n",
       " 'V139',\n",
       " 'V140',\n",
       " 'V141',\n",
       " 'V142',\n",
       " 'V143',\n",
       " 'V144',\n",
       " 'V145',\n",
       " 'V146',\n",
       " 'V147',\n",
       " 'V148',\n",
       " 'V149',\n",
       " 'V150',\n",
       " 'V151',\n",
       " 'V152',\n",
       " 'V153',\n",
       " 'V154',\n",
       " 'V155',\n",
       " 'V156',\n",
       " 'V157',\n",
       " 'V158',\n",
       " 'V159',\n",
       " 'V160',\n",
       " 'V161',\n",
       " 'V162',\n",
       " 'V163',\n",
       " 'V164',\n",
       " 'V165',\n",
       " 'V166',\n",
       " 'V167',\n",
       " 'V168',\n",
       " 'V169',\n",
       " 'V170',\n",
       " 'V171',\n",
       " 'V172',\n",
       " 'V173',\n",
       " 'V174',\n",
       " 'V175',\n",
       " 'V176',\n",
       " 'V177',\n",
       " 'V178',\n",
       " 'V179',\n",
       " 'V180',\n",
       " 'V181',\n",
       " 'V182',\n",
       " 'V183',\n",
       " 'V184',\n",
       " 'V185',\n",
       " 'V186',\n",
       " 'V187',\n",
       " 'V188',\n",
       " 'V189',\n",
       " 'V190',\n",
       " 'V191',\n",
       " 'V192',\n",
       " 'V193',\n",
       " 'V194',\n",
       " 'V195',\n",
       " 'V196',\n",
       " 'V197',\n",
       " 'V198',\n",
       " 'V199',\n",
       " 'V200',\n",
       " 'V201',\n",
       " 'V202',\n",
       " 'V203',\n",
       " 'V204',\n",
       " 'V205',\n",
       " 'V206',\n",
       " 'V207',\n",
       " 'V208',\n",
       " 'V209',\n",
       " 'V210',\n",
       " 'V211',\n",
       " 'V212',\n",
       " 'V213',\n",
       " 'V214',\n",
       " 'V215',\n",
       " 'V216',\n",
       " 'V217',\n",
       " 'V218',\n",
       " 'V219',\n",
       " 'V220',\n",
       " 'V221',\n",
       " 'V222',\n",
       " 'V223',\n",
       " 'V224',\n",
       " 'V225',\n",
       " 'V226',\n",
       " 'V227',\n",
       " 'V228',\n",
       " 'V229',\n",
       " 'V230',\n",
       " 'V231',\n",
       " 'V232',\n",
       " 'V233',\n",
       " 'V234',\n",
       " 'V235',\n",
       " 'V236',\n",
       " 'V237',\n",
       " 'V238',\n",
       " 'V239',\n",
       " 'V240',\n",
       " 'V241',\n",
       " 'V242',\n",
       " 'V243',\n",
       " 'V244',\n",
       " 'V245',\n",
       " 'V246',\n",
       " 'V247',\n",
       " 'V248',\n",
       " 'V249',\n",
       " 'V250',\n",
       " 'V251',\n",
       " 'V252',\n",
       " 'V253',\n",
       " 'V254',\n",
       " 'V255',\n",
       " 'V256',\n",
       " 'V257',\n",
       " 'V258',\n",
       " 'V259',\n",
       " 'V260',\n",
       " 'V261',\n",
       " 'V262',\n",
       " 'V263',\n",
       " 'V264',\n",
       " 'V265',\n",
       " 'V266',\n",
       " 'V267',\n",
       " 'V268',\n",
       " 'V269',\n",
       " 'V270',\n",
       " 'V271',\n",
       " 'V272',\n",
       " 'V273',\n",
       " 'V274',\n",
       " 'V275',\n",
       " 'V276',\n",
       " 'V277',\n",
       " 'V278',\n",
       " 'V279',\n",
       " 'V280',\n",
       " 'V281',\n",
       " 'V282',\n",
       " 'V283',\n",
       " 'V284',\n",
       " 'V285',\n",
       " 'V286',\n",
       " 'V287',\n",
       " 'V288',\n",
       " 'V289',\n",
       " 'V290',\n",
       " 'V291',\n",
       " 'V292',\n",
       " 'V293',\n",
       " 'V294',\n",
       " 'V295',\n",
       " 'V296',\n",
       " 'V297',\n",
       " 'V298',\n",
       " 'V299',\n",
       " 'V300',\n",
       " 'V301',\n",
       " 'V302',\n",
       " 'V303',\n",
       " 'V304',\n",
       " 'V305',\n",
       " 'V306',\n",
       " 'V307',\n",
       " 'V308',\n",
       " 'V309',\n",
       " 'V310',\n",
       " 'V311',\n",
       " 'V312',\n",
       " 'V313',\n",
       " 'V314',\n",
       " 'V315',\n",
       " 'V316',\n",
       " 'V317',\n",
       " 'V318',\n",
       " 'V319',\n",
       " 'V320',\n",
       " 'V321',\n",
       " 'V322',\n",
       " 'V323',\n",
       " 'V324',\n",
       " 'V325',\n",
       " 'V326',\n",
       " 'V327',\n",
       " 'V328',\n",
       " 'V329',\n",
       " 'V330',\n",
       " 'V331',\n",
       " 'V332',\n",
       " 'V333',\n",
       " 'V334',\n",
       " 'V335',\n",
       " 'V336',\n",
       " 'V337',\n",
       " 'V338',\n",
       " 'V339',\n",
       " 'card1_V307_mean',\n",
       " 'card1_V307_std',\n",
       " 'card1_V307_max',\n",
       " 'card1_V307_min',\n",
       " 'card1_V307_median',\n",
       " 'card1_V307_nunique',\n",
       " 'card1_V307_skew',\n",
       " 'card2_V307_mean',\n",
       " 'card2_V307_std',\n",
       " 'card2_V307_max',\n",
       " 'card2_V307_min',\n",
       " 'card2_V307_median',\n",
       " 'card2_V307_nunique',\n",
       " 'card2_V307_skew',\n",
       " 'card3_V307_mean',\n",
       " 'card3_V307_std',\n",
       " 'card3_V307_max',\n",
       " 'card3_V307_min',\n",
       " 'card3_V307_median',\n",
       " 'card3_V307_nunique',\n",
       " 'card3_V307_skew',\n",
       " 'card5_V307_mean',\n",
       " 'card5_V307_std',\n",
       " 'card5_V307_max',\n",
       " 'card5_V307_min',\n",
       " 'card5_V307_median',\n",
       " 'card5_V307_nunique',\n",
       " 'card5_V307_skew',\n",
       " 'uid_V307_mean',\n",
       " 'uid_V307_std',\n",
       " 'uid_V307_max',\n",
       " 'uid_V307_min',\n",
       " 'uid_V307_median',\n",
       " 'uid_V307_nunique',\n",
       " 'uid_V307_skew',\n",
       " 'uid2_V307_mean',\n",
       " 'uid2_V307_std',\n",
       " 'uid2_V307_max',\n",
       " 'uid2_V307_min',\n",
       " 'uid2_V307_median',\n",
       " 'uid2_V307_nunique',\n",
       " 'uid2_V307_skew',\n",
       " 'uid3_V307_mean',\n",
       " 'uid3_V307_std',\n",
       " 'uid3_V307_max',\n",
       " 'uid3_V307_min',\n",
       " 'uid3_V307_median',\n",
       " 'uid3_V307_nunique',\n",
       " 'uid3_V307_skew',\n",
       " 'uid4_V307_mean',\n",
       " 'uid4_V307_std',\n",
       " 'uid4_V307_max',\n",
       " 'uid4_V307_min',\n",
       " 'uid4_V307_median',\n",
       " 'uid4_V307_nunique',\n",
       " 'uid4_V307_skew',\n",
       " 'uid5_V307_mean',\n",
       " 'uid5_V307_std',\n",
       " 'uid5_V307_max',\n",
       " 'uid5_V307_min',\n",
       " 'uid5_V307_median',\n",
       " 'uid5_V307_nunique',\n",
       " 'uid5_V307_skew',\n",
       " 'addr1_V307_mean',\n",
       " 'addr1_V307_std',\n",
       " 'addr1_V307_max',\n",
       " 'addr1_V307_min',\n",
       " 'addr1_V307_median',\n",
       " 'addr1_V307_nunique',\n",
       " 'addr1_V307_skew',\n",
       " 'uid7_V307_mean',\n",
       " 'uid7_V307_std',\n",
       " 'uid7_V307_max',\n",
       " 'uid7_V307_min',\n",
       " 'uid7_V307_median',\n",
       " 'uid7_V307_nunique',\n",
       " 'uid7_V307_skew',\n",
       " 'uid8_V307_mean',\n",
       " 'uid8_V307_std',\n",
       " 'uid8_V307_max',\n",
       " 'uid8_V307_min',\n",
       " 'uid8_V307_median',\n",
       " 'uid8_V307_nunique',\n",
       " 'uid8_V307_skew',\n",
       " 'card1_TransactionAmt_mean',\n",
       " 'card1_TransactionAmt_std',\n",
       " 'card1_TransactionAmt_max',\n",
       " 'card1_TransactionAmt_min',\n",
       " 'card1_TransactionAmt_median',\n",
       " 'card1_TransactionAmt_nunique',\n",
       " 'card1_TransactionAmt_skew',\n",
       " 'card2_TransactionAmt_mean',\n",
       " 'card2_TransactionAmt_std',\n",
       " 'card2_TransactionAmt_max',\n",
       " 'card2_TransactionAmt_min',\n",
       " 'card2_TransactionAmt_median',\n",
       " 'card2_TransactionAmt_nunique',\n",
       " 'card2_TransactionAmt_skew',\n",
       " 'card3_TransactionAmt_mean',\n",
       " 'card3_TransactionAmt_std',\n",
       " 'card3_TransactionAmt_max',\n",
       " 'card3_TransactionAmt_min',\n",
       " 'card3_TransactionAmt_median',\n",
       " 'card3_TransactionAmt_nunique',\n",
       " 'card3_TransactionAmt_skew',\n",
       " 'card5_TransactionAmt_mean',\n",
       " 'card5_TransactionAmt_std',\n",
       " 'card5_TransactionAmt_max',\n",
       " 'card5_TransactionAmt_min',\n",
       " 'card5_TransactionAmt_median',\n",
       " 'card5_TransactionAmt_nunique',\n",
       " 'card5_TransactionAmt_skew',\n",
       " 'uid_TransactionAmt_mean',\n",
       " 'uid_TransactionAmt_std',\n",
       " 'uid_TransactionAmt_max',\n",
       " 'uid_TransactionAmt_min',\n",
       " 'uid_TransactionAmt_median',\n",
       " 'uid_TransactionAmt_nunique',\n",
       " 'uid_TransactionAmt_skew',\n",
       " 'uid2_TransactionAmt_mean',\n",
       " 'uid2_TransactionAmt_std',\n",
       " 'uid2_TransactionAmt_max',\n",
       " 'uid2_TransactionAmt_min',\n",
       " 'uid2_TransactionAmt_median',\n",
       " 'uid2_TransactionAmt_nunique',\n",
       " 'uid2_TransactionAmt_skew',\n",
       " 'uid3_TransactionAmt_mean',\n",
       " 'uid3_TransactionAmt_std',\n",
       " 'uid3_TransactionAmt_max',\n",
       " 'uid3_TransactionAmt_min',\n",
       " 'uid3_TransactionAmt_median',\n",
       " 'uid3_TransactionAmt_nunique',\n",
       " 'uid3_TransactionAmt_skew',\n",
       " 'uid4_TransactionAmt_mean',\n",
       " 'uid4_TransactionAmt_std',\n",
       " 'uid4_TransactionAmt_max',\n",
       " 'uid4_TransactionAmt_min',\n",
       " 'uid4_TransactionAmt_median',\n",
       " 'uid4_TransactionAmt_nunique',\n",
       " 'uid4_TransactionAmt_skew',\n",
       " 'uid5_TransactionAmt_mean',\n",
       " 'uid5_TransactionAmt_std',\n",
       " 'uid5_TransactionAmt_max',\n",
       " 'uid5_TransactionAmt_min',\n",
       " 'uid5_TransactionAmt_median',\n",
       " 'uid5_TransactionAmt_nunique',\n",
       " 'uid5_TransactionAmt_skew',\n",
       " 'addr1_TransactionAmt_mean',\n",
       " 'addr1_TransactionAmt_std',\n",
       " 'addr1_TransactionAmt_max',\n",
       " 'addr1_TransactionAmt_min',\n",
       " 'addr1_TransactionAmt_median',\n",
       " 'addr1_TransactionAmt_nunique',\n",
       " 'addr1_TransactionAmt_skew',\n",
       " 'uid7_TransactionAmt_mean',\n",
       " 'uid7_TransactionAmt_std',\n",
       " 'uid7_TransactionAmt_max',\n",
       " 'uid7_TransactionAmt_min',\n",
       " 'uid7_TransactionAmt_median',\n",
       " 'uid7_TransactionAmt_nunique',\n",
       " 'uid7_TransactionAmt_skew',\n",
       " 'uid8_TransactionAmt_mean',\n",
       " 'uid8_TransactionAmt_std',\n",
       " 'uid8_TransactionAmt_min',\n",
       " 'uid8_TransactionAmt_median',\n",
       " 'uid8_TransactionAmt_skew',\n",
       " 'card1_D15_mean',\n",
       " 'card1_D15_std',\n",
       " 'card1_D15_max',\n",
       " 'card1_D15_min',\n",
       " 'card1_D15_median',\n",
       " 'card1_D15_nunique',\n",
       " 'card1_D15_skew',\n",
       " 'card2_D15_mean',\n",
       " 'card2_D15_std',\n",
       " 'card2_D15_max',\n",
       " 'card2_D15_min',\n",
       " 'card2_D15_median',\n",
       " 'card2_D15_nunique',\n",
       " 'card2_D15_skew',\n",
       " 'card3_D15_mean',\n",
       " 'card3_D15_std',\n",
       " 'card3_D15_max',\n",
       " 'card3_D15_min',\n",
       " 'card3_D15_median',\n",
       " 'card3_D15_nunique',\n",
       " 'card3_D15_skew',\n",
       " 'card5_D15_mean',\n",
       " 'card5_D15_std',\n",
       " 'card5_D15_max',\n",
       " 'card5_D15_min',\n",
       " 'card5_D15_median',\n",
       " 'card5_D15_nunique',\n",
       " 'card5_D15_skew',\n",
       " 'uid_D15_mean',\n",
       " 'uid_D15_std',\n",
       " 'uid_D15_max',\n",
       " 'uid_D15_min',\n",
       " 'uid_D15_median',\n",
       " 'uid_D15_nunique',\n",
       " 'uid_D15_skew',\n",
       " 'uid2_D15_mean',\n",
       " 'uid2_D15_std',\n",
       " 'uid2_D15_max',\n",
       " 'uid2_D15_min',\n",
       " 'uid2_D15_median',\n",
       " 'uid2_D15_nunique',\n",
       " 'uid2_D15_skew',\n",
       " 'uid3_D15_mean',\n",
       " 'uid3_D15_std',\n",
       " 'uid3_D15_max',\n",
       " 'uid3_D15_min',\n",
       " 'uid3_D15_median',\n",
       " 'uid3_D15_nunique',\n",
       " 'uid3_D15_skew',\n",
       " 'uid4_D15_std',\n",
       " 'uid4_D15_max',\n",
       " 'uid4_D15_min',\n",
       " 'uid4_D15_nunique',\n",
       " 'uid4_D15_skew',\n",
       " 'uid5_D15_std',\n",
       " 'uid5_D15_skew',\n",
       " 'addr1_D15_mean',\n",
       " 'addr1_D15_std',\n",
       " 'addr1_D15_max',\n",
       " 'addr1_D15_min',\n",
       " 'addr1_D15_median',\n",
       " 'addr1_D15_nunique',\n",
       " 'addr1_D15_skew',\n",
       " 'uid7_D15_skew',\n",
       " 'uid8_D15_skew',\n",
       " 'card1_dist1_mean',\n",
       " 'card1_dist1_std',\n",
       " 'card1_dist1_max',\n",
       " 'card1_dist1_min',\n",
       " 'card1_dist1_median',\n",
       " 'card1_dist1_nunique',\n",
       " 'card1_dist1_skew',\n",
       " 'card2_dist1_mean',\n",
       " 'card2_dist1_std',\n",
       " 'card2_dist1_max',\n",
       " 'card2_dist1_min',\n",
       " 'card2_dist1_median',\n",
       " 'card2_dist1_nunique',\n",
       " 'card2_dist1_skew',\n",
       " 'card3_dist1_mean',\n",
       " 'card3_dist1_std',\n",
       " 'card3_dist1_max',\n",
       " 'card3_dist1_min',\n",
       " 'card3_dist1_median',\n",
       " 'card3_dist1_nunique',\n",
       " 'card3_dist1_skew',\n",
       " 'card5_dist1_mean',\n",
       " 'card5_dist1_std',\n",
       " 'card5_dist1_max',\n",
       " 'card5_dist1_min',\n",
       " 'card5_dist1_median',\n",
       " 'card5_dist1_nunique',\n",
       " 'card5_dist1_skew',\n",
       " 'uid_dist1_mean',\n",
       " 'uid_dist1_std',\n",
       " 'uid_dist1_max',\n",
       " 'uid_dist1_min',\n",
       " 'uid_dist1_median',\n",
       " 'uid_dist1_nunique',\n",
       " 'uid_dist1_skew',\n",
       " 'uid2_dist1_mean',\n",
       " 'uid2_dist1_std',\n",
       " 'uid2_dist1_max',\n",
       " 'uid2_dist1_min',\n",
       " 'uid2_dist1_median',\n",
       " 'uid2_dist1_nunique',\n",
       " 'uid2_dist1_skew',\n",
       " 'uid3_dist1_mean',\n",
       " 'uid3_dist1_std',\n",
       " 'uid3_dist1_max',\n",
       " 'uid3_dist1_min',\n",
       " 'uid3_dist1_median',\n",
       " 'uid3_dist1_nunique',\n",
       " 'uid3_dist1_skew',\n",
       " 'uid4_dist1_mean',\n",
       " 'uid4_dist1_std',\n",
       " 'uid4_dist1_max',\n",
       " 'uid4_dist1_min',\n",
       " 'uid4_dist1_median',\n",
       " 'uid4_dist1_nunique',\n",
       " 'uid4_dist1_skew',\n",
       " 'uid5_dist1_mean',\n",
       " 'uid5_dist1_std',\n",
       " 'uid5_dist1_max',\n",
       " 'uid5_dist1_min',\n",
       " 'uid5_dist1_median',\n",
       " 'uid5_dist1_nunique',\n",
       " 'uid5_dist1_skew',\n",
       " 'addr1_dist1_mean',\n",
       " 'addr1_dist1_std',\n",
       " 'addr1_dist1_max',\n",
       " 'addr1_dist1_min',\n",
       " 'addr1_dist1_median',\n",
       " 'addr1_dist1_nunique',\n",
       " 'addr1_dist1_skew',\n",
       " 'uid7_dist1_std',\n",
       " 'email_check',\n",
       " 'R_emaildomain_prefix2',\n",
       " 'P_emaildomain_prefix2',\n",
       " 'id_01',\n",
       " 'id_02',\n",
       " 'id_04',\n",
       " 'id_05',\n",
       " 'id_06',\n",
       " 'id_07',\n",
       " 'id_12',\n",
       " 'id_13',\n",
       " 'id_14',\n",
       " 'id_16',\n",
       " 'id_17',\n",
       " 'id_19',\n",
       " 'id_20',\n",
       " 'id_26',\n",
       " 'id_28',\n",
       " 'id_29',\n",
       " 'id_32',\n",
       " 'id_34',\n",
       " 'id_35',\n",
       " 'id_36',\n",
       " 'id_37',\n",
       " 'id_38',\n",
       " 'DeviceType',\n",
       " 'DeviceInfo',\n",
       " 'id_33_0',\n",
       " 'id_33_1',\n",
       " 'is_na_sum2',\n",
       " 'not_na_sum2',\n",
       " 'id_30_device',\n",
       " 'UserAgent',\n",
       " 'card1_fq_enc',\n",
       " 'card2_fq_enc',\n",
       " 'card3_fq_enc',\n",
       " 'card5_fq_enc',\n",
       " 'C1_fq_enc',\n",
       " 'C2_fq_enc',\n",
       " 'C3_fq_enc',\n",
       " 'C4_fq_enc',\n",
       " 'C5_fq_enc',\n",
       " 'C6_fq_enc',\n",
       " 'C7_fq_enc',\n",
       " 'C8_fq_enc',\n",
       " 'C9_fq_enc',\n",
       " 'C10_fq_enc',\n",
       " 'C11_fq_enc',\n",
       " 'C13_fq_enc',\n",
       " 'C14_fq_enc',\n",
       " 'D1_fq_enc',\n",
       " 'D2_fq_enc',\n",
       " 'D8_fq_enc',\n",
       " 'addr1_fq_enc',\n",
       " 'addr2_fq_enc',\n",
       " 'dist1_fq_enc',\n",
       " 'P_emaildomain_fq_enc',\n",
       " 'R_emaildomain_prefix2_fq_enc',\n",
       " 'P_emaildomain_prefix2_fq_enc',\n",
       " 'DeviceInfo_device_fq_enc',\n",
       " 'DeviceInfo_version_fq_enc',\n",
       " 'id_30_fq_enc',\n",
       " 'id_30_device_fq_enc',\n",
       " 'id_30_version_fq_enc',\n",
       " 'id_31_device_fq_enc',\n",
       " 'id_33_fq_enc',\n",
       " 'uid_fq_enc',\n",
       " 'uid2_fq_enc',\n",
       " 'uid3_fq_enc',\n",
       " 'uid4_fq_enc',\n",
       " 'uid5_fq_enc',\n",
       " 'uid7_fq_enc',\n",
       " 'uid8_fq_enc',\n",
       " 'uid_DT_M']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_df[features_columns].to_pickle(r\"D:\\学习\\data_mining\\IEEE-CIS_Fraud_Detection\\kernels\\feature_selection\\train.pkl\")\n",
    "# test_df[features_columns].to_pickle(r\"D:\\学习\\data_mining\\IEEE-CIS_Fraud_Detection\\kernels\\feature_selection\\test.pkl\")\n",
    "# train_df[TARGET].to_pickle(r\"D:\\学习\\data_mining\\IEEE-CIS_Fraud_Detection\\kernels\\feature_selection\\target.pkl\")\n",
    "# train_df[TARGET].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "lgb_params = {\n",
    "                    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':0.7,\n",
    "                    'n_estimators':800,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100, \n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scale_pos_weight说我觉得我还可以再拯救一下\n",
    "#train_labels = train_df[TARGET]\n",
    "#ratio= 0.2 * float(np.sum(train_labels == 0)) / np.sum(train_labels == 1)\n",
    "#ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lgb_params['scale_pos_weight'] = ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "456201 134339\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.956852\tvalid_1's auc: 0.889499\n",
      "[400]\ttraining's auc: 0.977332\tvalid_1's auc: 0.902315\n",
      "[600]\ttraining's auc: 0.989096\tvalid_1's auc: 0.909743\n",
      "[800]\ttraining's auc: 0.9949\tvalid_1's auc: 0.916009\n",
      "[1000]\ttraining's auc: 0.997517\tvalid_1's auc: 0.919879\n",
      "[1200]\ttraining's auc: 0.998786\tvalid_1's auc: 0.922152\n",
      "[1400]\ttraining's auc: 0.999405\tvalid_1's auc: 0.924189\n",
      "[1600]\ttraining's auc: 0.9997\tvalid_1's auc: 0.925373\n",
      "[1800]\ttraining's auc: 0.999844\tvalid_1's auc: 0.926217\n",
      "[2000]\ttraining's auc: 0.999922\tvalid_1's auc: 0.926574\n",
      "Early stopping, best iteration is:\n",
      "[2081]\ttraining's auc: 0.999941\tvalid_1's auc: 0.926757\n",
      "Fold: 1\n",
      "488572 101968\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.953476\tvalid_1's auc: 0.918497\n",
      "[400]\ttraining's auc: 0.976301\tvalid_1's auc: 0.9327\n",
      "[600]\ttraining's auc: 0.988981\tvalid_1's auc: 0.940994\n",
      "[800]\ttraining's auc: 0.995092\tvalid_1's auc: 0.946117\n",
      "[1000]\ttraining's auc: 0.997771\tvalid_1's auc: 0.948683\n",
      "[1200]\ttraining's auc: 0.998948\tvalid_1's auc: 0.950394\n",
      "[1400]\ttraining's auc: 0.999467\tvalid_1's auc: 0.951236\n",
      "[1600]\ttraining's auc: 0.999724\tvalid_1's auc: 0.951717\n",
      "[1800]\ttraining's auc: 0.999857\tvalid_1's auc: 0.952058\n",
      "[2000]\ttraining's auc: 0.999927\tvalid_1's auc: 0.952298\n",
      "[2200]\ttraining's auc: 0.999964\tvalid_1's auc: 0.952438\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2200]\ttraining's auc: 0.999964\tvalid_1's auc: 0.952438\n",
      "Fold: 2\n",
      "498030 92510\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.95207\tvalid_1's auc: 0.910864\n",
      "[400]\ttraining's auc: 0.974401\tvalid_1's auc: 0.926183\n",
      "[600]\ttraining's auc: 0.987785\tvalid_1's auc: 0.936481\n",
      "[800]\ttraining's auc: 0.994412\tvalid_1's auc: 0.943072\n",
      "[1000]\ttraining's auc: 0.997388\tvalid_1's auc: 0.946546\n",
      "[1200]\ttraining's auc: 0.998707\tvalid_1's auc: 0.948721\n",
      "[1400]\ttraining's auc: 0.999335\tvalid_1's auc: 0.950279\n",
      "[1600]\ttraining's auc: 0.999649\tvalid_1's auc: 0.951369\n",
      "[1800]\ttraining's auc: 0.999814\tvalid_1's auc: 0.952192\n",
      "[2000]\ttraining's auc: 0.999903\tvalid_1's auc: 0.952758\n",
      "[2200]\ttraining's auc: 0.999951\tvalid_1's auc: 0.953254\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2200]\ttraining's auc: 0.999951\tvalid_1's auc: 0.953254\n",
      "Fold: 3\n",
      "500867 89673\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.949801\tvalid_1's auc: 0.911575\n",
      "[400]\ttraining's auc: 0.974331\tvalid_1's auc: 0.927625\n",
      "[600]\ttraining's auc: 0.987818\tvalid_1's auc: 0.938191\n",
      "[800]\ttraining's auc: 0.994241\tvalid_1's auc: 0.944647\n",
      "[1000]\ttraining's auc: 0.997243\tvalid_1's auc: 0.948071\n",
      "[1200]\ttraining's auc: 0.998665\tvalid_1's auc: 0.949893\n",
      "[1400]\ttraining's auc: 0.999315\tvalid_1's auc: 0.950775\n",
      "[1600]\ttraining's auc: 0.999634\tvalid_1's auc: 0.951448\n",
      "[1800]\ttraining's auc: 0.999794\tvalid_1's auc: 0.95176\n",
      "Early stopping, best iteration is:\n",
      "[1877]\ttraining's auc: 0.999836\tvalid_1's auc: 0.95192\n",
      "Fold: 4\n",
      "504815 85725\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.950143\tvalid_1's auc: 0.933449\n",
      "[400]\ttraining's auc: 0.973949\tvalid_1's auc: 0.94506\n",
      "[600]\ttraining's auc: 0.987372\tvalid_1's auc: 0.950935\n",
      "[800]\ttraining's auc: 0.994052\tvalid_1's auc: 0.954248\n",
      "[1000]\ttraining's auc: 0.997134\tvalid_1's auc: 0.956275\n",
      "[1200]\ttraining's auc: 0.998564\tvalid_1's auc: 0.957061\n",
      "[1400]\ttraining's auc: 0.999246\tvalid_1's auc: 0.957426\n",
      "[1600]\ttraining's auc: 0.999594\tvalid_1's auc: 0.957559\n",
      "Early stopping, best iteration is:\n",
      "[1673]\ttraining's auc: 0.999677\tvalid_1's auc: 0.957665\n",
      "Fold: 5\n",
      "504215 86325\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.948874\tvalid_1's auc: 0.918694\n",
      "[400]\ttraining's auc: 0.973904\tvalid_1's auc: 0.938309\n",
      "[600]\ttraining's auc: 0.987499\tvalid_1's auc: 0.94866\n",
      "[800]\ttraining's auc: 0.994114\tvalid_1's auc: 0.95427\n",
      "[1000]\ttraining's auc: 0.997129\tvalid_1's auc: 0.957702\n",
      "[1200]\ttraining's auc: 0.99856\tvalid_1's auc: 0.959888\n",
      "[1400]\ttraining's auc: 0.99925\tvalid_1's auc: 0.961078\n",
      "[1600]\ttraining's auc: 0.999597\tvalid_1's auc: 0.961821\n",
      "[1800]\ttraining's auc: 0.99978\tvalid_1's auc: 0.962381\n",
      "[2000]\ttraining's auc: 0.999881\tvalid_1's auc: 0.962752\n",
      "[2200]\ttraining's auc: 0.999936\tvalid_1's auc: 0.962902\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2200]\ttraining's auc: 0.999936\tvalid_1's auc: 0.962902\n",
      "OOF AUC: 0.9499012720107367\n",
      "Wall time: 2h 10min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "########################### Model Train\n",
    "if LOCAL_TEST:\n",
    "    lgb_params['learning_rate'] = 0.01\n",
    "    lgb_params['n_estimators'] = 4000\n",
    "    lgb_params['early_stopping_rounds'] = 100\n",
    "    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "else:\n",
    "    lgb_params['learning_rate'] = 0.005\n",
    "    lgb_params['n_estimators'] = 2200\n",
    "    lgb_params['early_stopping_rounds'] = 100    \n",
    "    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params, NFOLDS=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### Export\n",
    "if not LOCAL_TEST:\n",
    "    test_predictions['isFraud'] = test_predictions['prediction']\n",
    "    test_predictions[['TransactionID','isFraud']].to_csv('middle_result\\submission_lgmb_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['P_emaildomain','R_emaildomain','uid','uid2','uid3','uid4','uid5','uid7','uid8','R_emaildomain_prefix2','P_emaildomain_prefix2','id_30','id_31','DeviceType','DeviceInfo','DeviceInfo_version','id_30_device','id_30_version','id_31_device','UserAgent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in features:\n",
    "    train_df[col] = train_df[col].astype('category')\n",
    "    test_df[col] = test_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle('data\\train_final.pkl')\n",
    "test_df.to_pickle('data\\test_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
